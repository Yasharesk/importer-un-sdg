{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import CSVs into OWID Grapher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook contains code for importing the UN SDG database into Grapher. It consumes the CSVs generated by a separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "  - The length of the `name` column in the `datasets` table had to be changed to `512`: we have a `UNIQUE` constraint on it and the `namespace` column, and I was getting duplicates: `alter table datasets change name name varchar(512);`\n",
    "  - We decided that this version of SDG needs to be imported into a new namespace (`un_sdg_2019`), as it was not possible to update the values of the existing version of SDG that is present in Grapher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from db import connection\n",
    "from db_utils import DBUtils\n",
    "\n",
    "from import_metadata import extract_description\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# ID of user who imported the data\n",
    "USER_ID = 29\n",
    "\n",
    "# Dataset namespace\n",
    "NAMESPACE = 'un_sdg_2019'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use OpenRefine to standardize the entities. [**Read the instructions in the lc-reconcile repo**](https://github.com/owid/lc-reconcile) to perform the standardization on the `entities.csv` file.\n",
    "\n",
    "At the end, you should have an `./entities_standardized.csv` file with columns:\n",
    "- `id` (as the original CSV)\n",
    "- `name` (as the original CSV)\n",
    "- `standardized_name` (the database entity name)\n",
    "- `db_entity_id` (the database entity id)\n",
    "\n",
    "**There will likely be entities that were not matched with a database entity, those will be inserted below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = pd.read_csv('./entities_standardized.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection as c:\n",
    "    db = DBUtils(c)\n",
    "    new_entities = entities[entities['db_entity_id'].isnull()]\n",
    "    for _, entity in new_entities.iterrows():\n",
    "        entity_id = entity.name\n",
    "        entity_name = entity['name']\n",
    "        db_entity_id = db.get_or_create_entity(entity_name)\n",
    "        entities.loc[entity_id, 'db_entity_id'] = db_entity_id\n",
    "        print(db_entity_id, entity['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entities[entities['db_entity_id'].isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [entity_id] â†’ [db_entity_id] lookup\n",
    "db_entity_id_by_entity_id = { row.name: int(row['db_entity_id']) for _, row in entities.iterrows() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = pd.read_csv('./exported_data/variables.csv')\n",
    "datasets = pd.read_csv('./exported_data/datasets.csv')\n",
    "sources = pd.read_csv('./exported_data/sources.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with connection as c:\n",
    "    db = DBUtils(c)\n",
    "    \n",
    "    for _, dataset in tqdm(datasets.iterrows(), total=len(datasets)):\n",
    "        \n",
    "        # Insert the dataset\n",
    "        print(\"Inserting dataset: %s\" % dataset['name'])\n",
    "        db_dataset_id = db.upsert_dataset(name=dataset['name'], namespace=NAMESPACE, user_id=USER_ID)\n",
    "        \n",
    "        # Insert the source\n",
    "        source = sources[sources['dataset_id'] == dataset.id].iloc[0]\n",
    "        print(\"Inserting source: %s\" % source['name'])\n",
    "        db_source_id = db.upsert_source(name=source['name'], description=source['description'], dataset_id=db_dataset_id)\n",
    "        \n",
    "        # Insert variables associated with this dataset\n",
    "        for j, variable in variables[variables.dataset_id == dataset['id']].iterrows():\n",
    "            # insert row in variables table\n",
    "            print(\"Inserting variable: %s\" % variable['name'])\n",
    "            db_variable_id = db.upsert_variable(\n",
    "                name=variable['name'], \n",
    "                code=None, \n",
    "                unit=variable['unit'], \n",
    "                short_unit=None, \n",
    "                source_id=db_source_id, \n",
    "                dataset_id=db_dataset_id, \n",
    "                description=None)\n",
    "\n",
    "            # read datapoints\n",
    "            data_values = pd.read_csv('./exported_data/%04d_datapoints.csv' % variable.id)\n",
    "\n",
    "            values = [(float(row['value']), int(row['year']), db_entity_id_by_entity_id[row['entity']], db_variable_id)\n",
    "                      for _, row in data_values.iterrows()]\n",
    "\n",
    "            print(\"Inserting values...\")\n",
    "            db.upsert_many(\"\"\"\n",
    "                INSERT INTO data_values (value, year, entityId, variableId)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "            \"\"\", values)\n",
    "            print(\"Inserted %d values for variable\" % len(values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful SQL for deleting all `data_values`, `variables`, `sources`, and `datasets` belonging to a `namespace`\n",
    "\n",
    "```sql\n",
    "DELETE data_values\n",
    "FROM   data_values\n",
    "       INNER JOIN variables\n",
    "               ON variables.id = data_values.variableid\n",
    "       INNER JOIN sources\n",
    "               ON sources.id = variables.sourceid\n",
    "       INNER JOIN datasets\n",
    "               ON datasets.id = sources.datasetid\n",
    "WHERE  datasets.namespace = 'un_sdg_2019';\n",
    "\n",
    "DELETE variables\n",
    "FROM   variables\n",
    "       INNER JOIN sources\n",
    "               ON sources.id = variables.sourceid\n",
    "       INNER JOIN datasets\n",
    "               ON datasets.id = sources.datasetid\n",
    "WHERE  datasets.namespace = 'un_sdg_2019';\n",
    "\n",
    "DELETE sources\n",
    "FROM   sources\n",
    "       INNER JOIN datasets\n",
    "               ON datasets.id = sources.datasetid\n",
    "WHERE  datasets.namespace = 'un_sdg_2019';\n",
    "\n",
    "DELETE FROM datasets\n",
    "WHERE  namespace = 'un_sdg_2019';  \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
