{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import CSVs into OWID Grapher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook contains code for importing the UN SDG database into Grapher. It consumes the CSVs generated by a separate notebook, and the PDFs associated with each indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "  - The length of the `name` column in the `datasets` table had to be changed to `512`: we have a `UNIQUE` constraint on it and the `namespace` column, and I was getting duplicates: `alter table datasets change name name varchar(512);`\n",
    "  - We decided that this version of SDG needs to be imported into a new namespace (`un_sdg_new`), as it was not possible to update the values of the existing version of SDG that is present in Grapher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from import_metadata import extract_description\n",
    "\n",
    "db = mysql.connector.connect(passwd=\"\",db=\"owid\", user=\"root\", host=\"127.0.0.1\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# ID of admin user (Max)\n",
    "USER_ID = 2\n",
    "\n",
    "source_description = {\n",
    "    'dataPublishedBy': \"United Nations Statistics Division\",\n",
    "    'dataPublisherSource': None,\n",
    "    'link': \"https://unstats.un.org/sdgs/indicators/database/\",\n",
    "    'retrievedDate': datetime.now().strftime(\"%d-%B-%y\"),\n",
    "    'additionalInfo': None\n",
    "}\n",
    "\n",
    "QUERY_INSERT_DATASET = \"\"\"\n",
    "INSERT INTO datasets (name, description, createdAt, updatedAt, namespace, createdByUserId, metadataEditedAt, metadataEditedByUserId, dataEditedAt, dataEditedByUserId)\n",
    "VALUES (%s, 'This is a dataset imported by the automated fetcher', NOW(), NOW(), 'un_sdg_new', %s, NOW(), %s, NOW(), %s)\n",
    "\"\"\"\n",
    "\n",
    "QUERY_INSERT_VARIABLE = \"\"\"\n",
    "INSERT INTO variables (name, unit, description, createdAt, updatedAt, coverage, \n",
    "                       timespan, datasetId, sourceId, display, columnOrder)\n",
    "VALUES (%s, %s, NULL, NOW(), NOW(), '', '', %s, %s, '{}', 0)\n",
    "\"\"\"\n",
    "\n",
    "QUERY_INSERT_SOURCE = \"\"\"\n",
    "INSERT INTO sources (name, description, createdAt, updatedAt, datasetId)\n",
    "VALUES (%s, %s, NOW(), NOW(), %s)\n",
    "\"\"\"\n",
    "\n",
    "QUERY_INSERT_DATAPOINTS = \"\"\"\n",
    "INSERT INTO data_values (value, year, entityId, variableId)\n",
    "VALUES (%s, %s, %s, %s)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the results of the pre-processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = pd.read_csv('./exported_data/variables.csv')\n",
    "datasets = pd.read_csv('./exported_data/datasets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in datasets.iterrows():\n",
    "    cursor = db.cursor()\n",
    "    additional_info = ''\n",
    "    try:\n",
    "        additional_info = extract_description('metadata/Metadata-%s.pdf' % '-'.join([part.rjust(2, '0') for part in dataset['Indicator'].split('.')]))\n",
    "    except:\n",
    "        print(\"Couldn't find metadata for indicator %s\" % dataset['Indicator'])\n",
    "        \n",
    "    try:\n",
    "        # insert row in datasets table\n",
    "        cursor.execute(QUERY_INSERT_DATASET, (\"%s %s\" % (dataset.Indicator, dataset.SeriesDescription), USER_ID, USER_ID, USER_ID))\n",
    "        new_dataset_id = cursor.lastrowid\n",
    "        print(\"Inserted dataset: %s\" % dataset.SeriesDescription)\n",
    "\n",
    "        source_description['additionalInfo'] = additional_info\n",
    "\n",
    "        # insert row in sources table\n",
    "        cursor.execute(QUERY_INSERT_SOURCE, \n",
    "                      (dataset.SeriesDescription[:250], json.dumps(source_description), new_dataset_id))\n",
    "        new_source_id = cursor.lastrowid\n",
    "\n",
    "        print(\"Inserted source: %d\" % new_source_id)\n",
    "\n",
    "        # Insert variables associated with this dataset\n",
    "        for j, variable in variables[(variables.Indicator == dataset.Indicator) & (variables.SeriesCode == dataset.SeriesCode)].iterrows():\n",
    "            # insert row in variables table\n",
    "            cursor.execute(QUERY_INSERT_VARIABLE, \n",
    "                          (variable.VariableDescription, variable.Units, new_dataset_id, new_source_id))\n",
    "            new_variable_id = cursor.lastrowid\n",
    "            print(\"Inserted variable: %s\" % variable.VariableDescription)\n",
    "\n",
    "            # read datapoints\n",
    "            print(\"Reading datapoints: %04d_datapoints.csv\" % variable.variable_idx)\n",
    "            data_values = pd.read_csv('./exported_data/%04d_datapoints.csv' % variable.variable_idx)\n",
    "            \n",
    "            # keep columns that matter\n",
    "            data_values = data_values[[\n",
    "                'Value',\n",
    "                'TimePeriod',\n",
    "                'owid_entity_id',\n",
    "            ]]\n",
    "            \n",
    "            # there are a few datasets that contain duplicates for the same country and year\n",
    "            # also, drop the ocassional NaN in any of the columns\n",
    "            # Clean them up\n",
    "            data_values = data_values.drop_duplicates(subset=['TimePeriod', 'owid_entity_id']).dropna()\n",
    "\n",
    "            values = [(value.Value, value.TimePeriod, value.owid_entity_id, new_variable_id)\n",
    "                      for _, value in data_values.iterrows()]\n",
    "\n",
    "            print(\"Inserting values...\")\n",
    "            cursor.executemany(QUERY_INSERT_DATAPOINTS, values)\n",
    "            print(\"Inserted %d values for variable\" % len(values))\n",
    "\n",
    "        db.commit()\n",
    "        cursor.close()\n",
    "        print(\"------\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        db.rollback()\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful SQL for deleting all `data_values`, `variables`, `sources`, and `datasets` belonging to a `namespace`\n",
    "\n",
    "```\n",
    "DELETE data_values\n",
    "FROM   data_values\n",
    "       INNER JOIN variables\n",
    "               ON variables.id = data_values.variableid\n",
    "       INNER JOIN sources\n",
    "               ON sources.id = variables.sourceid\n",
    "       INNER JOIN datasets\n",
    "               ON datasets.id = sources.datasetid\n",
    "WHERE  datasets.namespace = 'un_sdg_new';\n",
    "\n",
    "DELETE variables\n",
    "FROM   variables\n",
    "       INNER JOIN sources\n",
    "               ON sources.id = variables.sourceid\n",
    "       INNER JOIN datasets\n",
    "               ON datasets.id = sources.datasetid\n",
    "WHERE  datasets.namespace = 'un_sdg_new';\n",
    "\n",
    "DELETE sources\n",
    "FROM   sources\n",
    "       INNER JOIN datasets\n",
    "               ON datasets.id = sources.datasetid\n",
    "WHERE  datasets.namespace = 'un_sdg_new';\n",
    "\n",
    "DELETE FROM datasets\n",
    "WHERE  namespace = 'un_sdg_new';  \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
