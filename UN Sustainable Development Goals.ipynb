{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UN Sustainable Development Goals\n",
    "\n",
    "This notebook implements the pre-processing needed for importing the UN SDG dataset into OWID's grapher database.\n",
    "A rough outline of the process:\n",
    "\n",
    "  1. Read the dataset exported from the UN SDG Indicators database website [[1]](#Data-loading-and-preprocessing)\n",
    "  2. Export the referenced _entitites_ (geographic areas) [[2]](#Export-entities-(dimension-members))\n",
    "  3. _Reconcile_ those entities with OpenRefine and [OWID's geographic entities reconciliation service](https://github.com/owid/lc-reconcile/)\n",
    "  4. Generate a separate table for every combination distinct values of geographic entities and other nominal variables ([3](#Export-datasets-and-variables))\n",
    "  5. Export a `variables.csv` file, and a set of `dataset_*.csv` files that contains each generated table. [[4]](#Export-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import qgrid\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing\n",
    "\n",
    "The data was obtained from the [UN SDG Indicators database](https://unstats.un.org/sdgs/indicators/database). We selected all _Goals_ (topmost category in the classification of indicators) and requested the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"data/20190625214202728_manuel@jazzido.com_data.csv\", low_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the indicators that we care about (list taken from [the old importer](https://github.com/owid/owid-importer/blob/master/importer_django/un_sdg_importer.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDICATORS = [\n",
    "'1.1.1','1.2.1','1.3.1','1.5.1','1.5.2','1.5.3','2.1.1','2.1.2','2.2.1','2.2.2','2.5.1','2.5.2','2.a.1','2.a.2','2.c.1','3.1.1','3.1.2','3.2.1','3.2.2','3.3.1','3.3.2','3.3.3','3.3.5','3.4.1','3.4.2','3.5.2','3.6.1','3.7.1','3.7.2','3.9.1','3.9.2','3.9.3','3.a.1','3.b.2','3.c.1','3.d.1','4.1.1','4.2.1','4.2.2','4.3.1','4.4.1','4.5.1','4.6.1','4.a.1','4.b.1','4.c.1','5.2.1','5.3.1','5.3.2','5.4.1','5.5.1','5.5.2','5.6.1','5.b.1','6.1.1','6.2.1','6.4.2','6.5.1','6.a.1','6.b.1','7.1.1','7.1.2','7.2.1','7.3.1','8.1.1','8.2.1','8.3.1','8.4.1','8.4.2','8.5.1','8.5.2','8.6.1','8.7.1','8.8.1','8.10.1','8.10.2','8.a.1','9.1.2','9.2.1','9.2.2','9.4.1','9.5.1','9.5.2','9.a.1','9.b.1','9.c.1','10.1.1','10.4.1','10.6.1','10.a.1','10.b.1','10.c.1','11.1.1','11.5.1','11.5.2','11.6.1','11.6.2','11.b.1','12.2.1','12.2.2','12.4.1','13.1.1','13.1.2','14.4.1','14.5.1','15.1.1','15.1.2','15.2.1','15.4.1','15.4.2','15.5.1','15.6.1','15.a.1','15.b.1','16.1.1','16.2.1','16.2.2','16.2.3','16.3.2','16.5.2','16.8.1','16.9.1','16.10.1','16.10.2','16.a.1','17.2.1','17.3.2','17.4.1','17.6.2','17.8.1','17.9.1','17.10.1','17.11.1','17.12.1','17.15.1','17.16.1','17.18.2','17.18.3','17.19.1','17.19.2'\n",
    "]\n",
    "\n",
    "data = data[data.Indicator.isin(INDICATORS)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export entities (dimension members)\n",
    "\n",
    "We only deal with the _Geographic_ and _temporal_ dimensions. Produce a list of countries included in the SDG data file.\n",
    "\n",
    "This list of country names as they appear in the SDG dataset, will be reconciled through OWID's reconciler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_geo_areas = data[['GeoAreaCode', 'GeoAreaName']] \\\n",
    "    .drop_duplicates() \\\n",
    "    .rename(columns={'GeoAreaCode': 'id', 'GeoAreaName': 'name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_geo_areas.to_csv('./sdg_geo_areas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the output of the reconciliation process (saved as `sdg_owid_countries.csv`) and `merge` it with our data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_owid_countries = pd.read_csv('./sdg_owid_countries.csv')\n",
    "data = data.merge(sdg_owid_countries, left_on='GeoAreaCode', right_on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export datasets and variables\n",
    "\n",
    "Algorithm outline:\n",
    "\n",
    "  - For each `INDICATOR`:\n",
    "    - Obtain dimensions (columns named `[between brackets]`) that contain non-null values\n",
    "      - For each combination of unique values values in those dimensions\n",
    "        - Generate a table of values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSIONS = [c for c in data.columns if c[0] == '[' and c[-1] == ']']\n",
    "NON_DIMENSIONS = [c for c in data.columns if c not in set(DIMENSIONS)]\n",
    "\n",
    "@functools.lru_cache(maxsize=256)\n",
    "def get_series_with_relevant_dimensions(indicator, series):\n",
    "    \"\"\" For a given indicator and series, return a tuple:\n",
    "    \n",
    "      - data filtered to that indicator and series\n",
    "      - names of relevant dimensions\n",
    "      - unique values for each relevant dimension\n",
    "    \"\"\"\n",
    "    data_filtered = data[(data.Indicator == indicator) & (data.SeriesCode == series)]\n",
    "    non_null_dimensions_columns = [col for col in DIMENSIONS if data_filtered.loc[:, col].notna().any()]\n",
    "    dimension_names = []\n",
    "    dimension_unique_values = []\n",
    "    \n",
    "    for c in non_null_dimensions_columns:\n",
    "        uniques = data_filtered[c].unique()\n",
    "        if len(uniques) > 1:\n",
    "            dimension_names.append(c)\n",
    "            dimension_unique_values.append(list(uniques))\n",
    "\n",
    "    return (data_filtered[NON_DIMENSIONS + dimension_names], dimension_names, dimension_unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate tables for:\n",
    "\n",
    "  - Rows where the dimension is `None`\n",
    "  - One table for each combination of unique values of relevant dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=256)\n",
    "def generate_tables_for_indicator_and_series(indicator, series):\n",
    "    tables_by_combination = {}\n",
    "    data_filtered, dimensions, dimension_values = get_series_with_relevant_dimensions(indicator, series)\n",
    "    if len(dimensions) == 0:\n",
    "        # no additional dimensions\n",
    "        export = data_filtered\n",
    "        return export\n",
    "    else:\n",
    "        for dimension_value_combination in itertools.product(*dimension_values):\n",
    "            # build filter by reducing, start with a constant True boolean array\n",
    "            filt = [True] * len(data_filtered)\n",
    "            for dim_idx, dim_value in enumerate(dimension_value_combination):\n",
    "                dimension_name = dimensions[dim_idx]\n",
    "                value_is_nan = type(dim_value) == float and math.isnan(dim_value)\n",
    "                filt = filt \\\n",
    "                       & (data_filtered[dimension_name].isnull() if value_is_nan else data_filtered[dimension_name] == dim_value)\n",
    "\n",
    "            tables_by_combination[dimension_value_combination] = data_filtered[filt].drop(dimensions, axis=1)\n",
    "            \n",
    "        return tables_by_combination\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_series = data[['Indicator', 'SeriesCode', 'SeriesDescription', 'Units']] \\\n",
    "  .groupby(by=['Indicator', 'SeriesCode', 'SeriesDescription', 'Units']) \\\n",
    "  .count() \\\n",
    "  .reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export data\n",
    "\n",
    "For each series and combination of additional dimensions' members, generate an entry in the `variables` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_COLS_VARIABLES = ['Indicator', 'SeriesCode', 'VariableDescription', 'Units', 'variable_idx']\n",
    "DF_COLS_DATASETS = ['Indicator', 'SeriesCode', 'SeriesDescription']\n",
    "DF_COLS_DATAPOINTS = ['Value', 'TimePeriod', 'Time_Detail', 'Source', 'FootNote', 'Nature', 'owid_entity_id']\n",
    "variables = pd.DataFrame(columns=DF_COLS_VARIABLES)\n",
    "datasets = pd.DataFrame(columns=DF_COLS_DATASETS)\n",
    "\n",
    "variable_idx = 0\n",
    "\n",
    "for i, row in all_series.iterrows():\n",
    "    datasets = datasets.append(\n",
    "        {\n",
    "            'Indicator': row['Indicator'], \n",
    "            'SeriesCode': row['SeriesCode'], \n",
    "            'SeriesDescription': row['SeriesDescription']\n",
    "        }, \n",
    "        ignore_index=True)\n",
    "    _, dimensions, dimension_members = get_series_with_relevant_dimensions(row['Indicator'], row['SeriesCode'])\n",
    "    \n",
    "    if len(dimensions) == 0:\n",
    "        # no additional dimensions\n",
    "        table = generate_tables_for_indicator_and_series(row['Indicator'], row['SeriesCode'])\n",
    "        variable = { \n",
    "            'Indicator': row['Indicator'], 'SeriesCode': row['SeriesCode'], \n",
    "            'VariableDescription': row['SeriesDescription'], 'Units': row['Units'],\n",
    "            'variable_idx': variable_idx\n",
    "        }\n",
    "        variables = variables.append(variable, ignore_index=True)\n",
    "        table[DF_COLS_DATAPOINTS].to_csv('./exported_data/%04d_datapoints.csv' % variable_idx, index=False)\n",
    "        variable_idx += 1\n",
    "\n",
    "    else:\n",
    "        # has additional dimensions\n",
    "        for member_combination, table in generate_tables_for_indicator_and_series(row['Indicator'], row['SeriesCode']).items():\n",
    "            variable = { \n",
    "                'Indicator': row['Indicator'], 'SeriesCode': row['SeriesCode'], \n",
    "                'Units': row['Units'],\n",
    "                'VariableDescription': row['SeriesDescription'] + \" %s\" % ( ' - '.join(map(str, member_combination))),\n",
    "                'variable_idx': variable_idx\n",
    "            }\n",
    "            variables = variables.append(variable, ignore_index=True)\n",
    "            table[DF_COLS_DATAPOINTS].to_csv('./exported_data/%04d_datapoints.csv' % variable_idx, index=False)\n",
    "            variable_idx += 1\n",
    "\n",
    "\n",
    "variables.to_csv('./exported_data/variables.csv', index=False)\n",
    "datasets.to_csv('./exported_data/datasets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
